# -*- coding: utf-8 -*-
"""Recommendation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14q-n8Xv6xYr7S73-QO5b0lE7jNVeNzN4
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from geopy.geocoders import Nominatim
import geopandas as gpd
from shapely.geometry import Point
from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense
from tensorflow.keras.models import Model

class DataPreprocessor:
    def __init__(self):
        self.scaler = StandardScaler()

    def preprocess(self, df):
        # Handle missing values (example: fill missing zip codes with 'Unknown')
        df['zip_code'].fillna('Unknown', inplace=True)

        # Select features for clustering
        features = ['bed', 'bath', 'acre_lot', 'house_size']
        X = df[features]

        # Standardize the features
        X_scaled = self.scaler.fit_transform(X)

        # Add standardized features to the DataFrame
        df[f'{feature}_scaled' for feature in features] = X_scaled

        return df



class RecommendationSystem:
  def __init__(self, num_users, num_properties, embedding_dim=64):
        self.num_users = num_users
        self.num_properties = num_properties
        self.embedding_dim = embedding_dim

        # Define input layers
        user_input = Input(shape=(1,), dtype='int32', name='user_input')
        property_input = Input(shape=(1,), dtype='int32', name='property_input')

        # Embedding layers
        user_embedding = Embedding(num_users, embedding_dim)(user_input)
        property_embedding = Embedding(num_properties, embedding_dim)(property_input)

        # Flatten embeddings
        user_embedding = Flatten()(user_embedding)
        property_embedding = Flatten()(property_embedding)

        # Concatenate embeddings
        concatenated = Concatenate()([user_embedding, property_embedding])

        # Hidden layers (you can adjust the architecture)
        hidden1 = Dense(128, activation='relu')(concatenated)
        hidden2 = Dense(64, activation='relu')(hidden1)

        # Output layer
        output = Dense(1, activation='sigmoid')(hidden2)

        # Create the model
        self.model = Model(inputs=[user_input, property_input], outputs=output)

        # Compile the model
        self.model.compile(optimizer='adam', loss='binary_crossentropy')

    def train(self, user_input_train, property_input_train, ratings_train, epochs=10, batch_size=32):
        self.model.fit([user_input_train, property_input_train], ratings_train, epochs=epochs, batch_size=batch_size)

    def predict(self, user_input, property_input):
        return self.model.predict([user_input, property_input])

# Example usage
# (Assumes you have the necessary data preprocessing and model training steps)
# ...

# Use the trained model to make recommendations for a new user
# (Assuming the new user ID is 4)
new_user_id = 4
predicted_ratings = model.predict(

import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Embedding, Input, Flatten, Concatenate, Dense
from tensorflow.keras.models import Model

# Sample data (replace with your actual data)
data = {
    'user_id': [1, 1, 2, 2, 3, 3],
    'property_id': [1, 2, 1, 3, 2, 3],
    'rating': [5, 4, 3, 5, 1, 2]  
}
df = pd.DataFrame(data)

# Unique user and property IDs
user_ids = df['user_id'].unique()
property_ids = df['property_id'].unique()

# Create dictionaries to map IDs to indices
user_to_index = {user_id: i for i, user_id in enumerate(user_ids)}
property_to_index = {property_id: i for i, property_id in enumerate(property_ids)}

# Convert user and property IDs to indices
df['user_index'] = df['user_id'].map(user_to_index)
df['property_index'] = df['property_id'].map(property_to_index)

# Define model parameters
num_users = len(user_ids)
num_properties = len(property_ids)
embedding_dim = 64  # Dimension of embedding vectors

# Define input layers
user_input = Input(shape=(1,), dtype='int32', name='user_input')
property_input = Input(shape=(1,), dtype='int32', name='property_input')

# Embedding layers
user_embedding = Embedding(num_users, embedding_dim)(user_input)
property_embedding = Embedding(num_properties, embedding_dim)(property_input)

# Flatten embeddings
user_embedding = Flatten()(user_embedding)
property_embedding = Flatten()(property_embedding)

# Concatenate embeddings
concatenated = Concatenate()([user_embedding, property_embedding])

# Hidden layers (you can adjust the architecture)
hidden1 = Dense(128, activation='relu')(concatenated)
hidden2 = Dense(64, activation='relu')(hidden1)

# Output layer
output = Dense(1, activation='sigmoid')(hidden2)  # Predict interaction score (e.g., probability of liking)

# Create the model
model = Model(inputs=[user_input, property_input], outputs=output)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy')

# Prepare data for training
user_input_train = df['user_index'].values
property_input_train = df['property_index'].values
ratings_train = df['rating'].values


model.fit([user_input_train, property_input_train], ratings_train, epochs=10, batch_size=32)

